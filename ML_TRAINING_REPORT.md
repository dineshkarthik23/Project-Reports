# Deep Dive: Signal Machine Learning & Model Training

**Date:** February 6, 2026  
**Companion to:** `PROJECT_REPORT.md`  
**Focus:** Technical analysis of the Neural Network training, architecture evolution, and optimization.

---

## 1. The Core Objective

The goal of this Machine Learning system is to replicate the function of a digital modem using a neural network.
- **Traditional Modem:** Uses mathematical rules (QPSK, QAM) to map bits $\to$ symbols.
- **ML Modem:** "Learns" the optimal mapping (constellation) to survive channel noise.

**The Autoencoder Concept:**
$$ \text{Input (Bits)} \xrightarrow{\text{Encoder NN}} \text{Complex Symbols} \xrightarrow{\text{Channel (Noise)}} \text{Noisy Symbols} \xrightarrow{\text{Decoder NN}} \text{Output (Bits)} $$

---

## 2. Phase 1: The 8-Bit "Naive" Approach (Failure)

Our initial attempt tried to map a full byte (8 bits) directly to 4 complex symbols.

### Architecture (Attempt 1)
*   **Input Layer:** 8 neurons (one for each bit).
*   **Hidden Layers:**
    *   Dense(128, ReLU)
    *   Dense(64, ReLU)
    *   Dense(32, ReLU)
*   **Output (Encoder):** 8 neurons (representing Real/Imaginary parts of 4 symbols).
*   **Constraint:** Power Normalization (ensuring average energy = 1).

### Why It Failed (The "50% Accuracy" Trap)
Training stalled with an accuracy of ~0.50 (50%). This is mathematically equivalent to a random coin flip for each bit.

**Technical Reasons:**
1.  **Constellation Density:** Mapping 8 bits means creating $2^8 = 256$ distinct points in the signal space.
2.  **Vanishing Gradients:** With only 4 symbols to represent 256 states, the Euclidean distance between "neighbors" became microscopic. The noise (at 10dB SNR) was larger than the distance between points, making them indistinguishable.
3.  **Model Capacity:** A simplistic Multi-Layer Perceptron (MLP) with ~20k parameters lacked the geometric "intelligence" to separate 256 entangled clusters in high-dimensional space.

*Outcome: The model could not learn. The loss function remained flat.*

---

## 3. Phase 2: The 4-Bit "Smart" Pivot (Success)

We applied the engineering principle of **"simplify and conquer."** We reduced the problem scope to ensure convergence.

### Architecture Optimization (Current System)
We changed the input from 8 bits ($256$ states) to 4 bits ($16$ states).

*   **Input:** 4 bits (Values 0-15).
*   **Bottleneck:** 4 Complex Symbols (Same bandwidth as before).
*   **Sparsity:** We now only need to place 16 points in the same space that previously held 256. This increases the "minimum distance" ($d_{min}$) between points by factor of ~16x, making them highly robust to noise.

### Final Hyperparameters
| Parameter | Value | Reasoning |
| :--- | :--- | :--- |
| **Input Dimension** | 4 (Bits) | Reduced for guaranteed convergence. |
| **Latent Dim** | 4 (Complex Symbols) | $R=1$ bit/symbol (Robust rate). |
| **Hidden Layers** | 128 $\to$ 64 $\to$ 32 | Pyramidal compression structure. |
| **Activation** | ReLU | Fast convergence, avoids saturation. |
| **Output Alloc** | Sigmoid | Forces output bits to strictly 0 or 1 range. |
| **Loss Function** | Binary Crossentropy | Ideal for bit-flipping error minimization. |
| **Optimizer** | Adam (lr=0.001) | Adaptive learning rate for stability. |

---

## 4. Training Dynamics

The successful training of the 4-bit model showed classic convergence behavior.

### Training Environment
*   **Simulator:** TensorFlow 2.x
*   **Channel Model:** Additive White Gaussian Noise (AWGN)
*   **Training SNR:** 10.0 dB
*   **Batch Size:** 256 samples

### Progression (100 Epochs)
1.  **Epoch 1-5 (Chaos):** The model guesses randomly. Loss ~0.693 ($ln(2)$).
2.  **Epoch 10-20 (Organization):** The encoder learns to "explode" the constellation, pushing points as far apart as possible to satisfy the Power Normalization constraint.
3.  **Epoch 30-50 (Refinement):** The decoder learns the boundaries (Voronoi regions) between these distant points.
4.  **Epoch 100 (Convergence):**
    *   **Training Loss:** Near 0.0.
    *   **Validation Accuracy:** 100%.
    *   **Bit Error Rate (BER):** $0.000000$.

### Visualization
*   **Loss Curve:** Sharp exponential decay.
*   **Constellation:** A clear geometric structure emerged (likely a hyper-cube or rotated QAM-like structure), optimized purely by data, not human design.

---

## 5. Deployment: Separating the Brain

Once trained, the Autoencoder cannot be used "as is" because the transmitter and receiver are in different locations!

**The Surgery:**
1.  **Encoder Extraction:** The first half of the network (Input $\to$ `power_norm`) is sliced out and saved as `encoder.keras`. This becomes the **Transmitter**.
2.  **Decoder Extraction:** The second half (Channel Output $\to$ Final `sigmoid`) is sliced out and saved as `decoder.keras`. This becomes the **Receiver**.

**GNU Radio Integration:**
*   We use the **Encoder** to generate IQ samples from user files.
*   We use the **Decoder** to listen to the "air" (simulated or real), continuously predicting bits from incoming IQ streams.

---

## 6. Summary: Why This Works
By reducing the complexity to 4 bits, we allowed the Neural Network to find a **perfect global minimum** in the loss landscape. It effectively reinvented a robust modulation scheme (like BPSK/QPSK) on its own, optimized specifically for the 10dB AWGN channel we trained it on.

**Next Logic Step:** With the pipeline proven, we can now theoretically increase the model size (Deep Learning) to handle the original 8-bit goal, essentially "scaling up" the brain capacity to handle the harder task.

---
**Report generated by Antigravity AI**
